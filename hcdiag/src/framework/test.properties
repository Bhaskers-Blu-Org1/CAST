# -*- coding: utf-8 -*-                   # remove when Python 3
#================================================================================
#   
#    hcdiag/src/framework/test.properties
# 
#  Â© Copyright IBM Corporation 2015,2016. All Rights Reserved
#
#    This program is licensed under the terms of the Eclipse Public License
#    v1.0 as published by the Eclipse Foundation and available at
#    http://www.eclipse.org/legal/epl-v10.html
#
#    U.S. Government Users Restricted Rights:  Use, duplication or disclosure
#    restricted by GSA ADP Schedule Contract with IBM Corp.
# 
#===============================================================================*/


## chk-boot-time
[tests.chk-boot-time]
description = check if the nodes were boot succesfully 
executable  = /opt/ibm/csm/hcdiag/tests/chk-boot-time/chk-boot-time.sh
targetType  = Management
xcat_cmd    = yes
timeout     = 10
#args        = -v

## chk-cpu
[tests.chk-cpu]
description = Check number of cpus, cpu frequence, cores present=cores online on the node
executable  = /opt/ibm/csm/hcdiag/tests/chk-cpu/chk-cpu.sh
timeout     = 10


## chk-cpu-count
[tests.chk-cpu-count]
description = check number of cpus on the node
executable  = /opt/ibm/csm/hcdiag/tests/chk-cpu-count/chk-cpu-count.sh
timeout     = 10

## chk-csm-health from CSM infrastructure
[tests.chk-csm-health]
description = csm infrastructure health check
executable  = /opt/ibm/csm/hcdiag/tests/chk-csm-health/chk-csm-health.sh
timeout     = 100
targetType  = Management

# default threshold is 10% of the available memory
[tests.chk-free-memory]
description = check if the amount of free memory is above the threshold.
executable  = /opt/ibm/csm/hcdiag/tests/chk-free-memory/chk-free-memory.sh
timeout     = 10
#args = 10


## uses the clustconf.yaml as input for gpfs filesystem/mount point
## and check it with what is on the machine
[tests.chk-gpfs-mount]
description = check gpfs filesystem/mount point
executable  = /opt/ibm/csm/hcdiag/tests/chk-gpfs-mount/chk-gpfs-mount.sh
timeout     = 10

[tests.chk-hca-attributes]
description = check hca board version and firmware
executable  = /opt/ibm/csm/hcdiag/tests/chk-hca-attributes/chk-hca-attributes.sh
timeout     = 10

[tests.chk-ib-pcispeed]
description = check ib adapters vendor, speed and width
executable  = /opt/ibm/csm/hcdiag/tests/chk-ib-pcispeed/chk-ib-pcispeed.sh
timeout     = 10
#args       = 16GT/s x8 Mellanox

## dheck if there is a kworker process consuming more than N% of the cpu.
[tests.chk-kworker]
description = check kworker process consuming more than N% of the cpu
executable  = /opt/ibm/csm/hcdiag/tests/chk-kworker/chk-kworker.sh
#args        = 20.0

[tests.chk-led]
description = check for fault led in the node range 
executable  = /opt/ibm/csm/hcdiag/tests/chk-led/chk-led.sh
targetType  = Management
timeout     = 10

[tests.chk-load-average]
description = check if the node load average is below the threshold
executable  = /opt/ibm/csm/hcdiag/tests/chk-load/chk-load.sh
args        = average 40 
timeout     = 10

[tests.chk-load-cpu]
description = check if the node cpu load is bellow the threshold
executable  = /opt/ibm/csm/hcdiag/tests/chk-load/chk-load.sh
# args: [cpu] [threshold] [number_of_process]
#args        = cpu 90 1
timeout     = 10

[tests.chk-load-mem]
description = check if the node memory load is bellow the threshold
executable  = /opt/ibm/csm/hcdiag/tests/chk-load/chk-load.sh
# args: mem [threshold] [number_of_process]
args        =  mem 90 1 
timeout     = 10

[tests.chk-memory]
description = check if the node total memory, number of banks and bank size
executable  = /opt/ibm/csm/hcdiag/tests/chk-memory/chk-memory.sh
timeout     = 10

## Check if the Mellanox adapters are configured correctly.
[tests.chk-mlnx-pci]
description = check mellanox pci configuration 
executable  = /opt/ibm/csm/hcdiag/tests/chk-mlnx-pci/chk-mlnx-pci.sh
timeout     = 10

## chk-noping
[tests.chk-noping]
description = check if the nodes that are booted are pinging
executable  = /opt/ibm/csm/hcdiag/tests/chk-noping/chk-noping.sh
targetType  = Management
xcat_cmd    = yes
timeout     = 100
#args        = -v

[tests.chk-nvidia-clocks]
description = check nvidia clocks
executable  = /opt/ibm/csm/hcdiag/tests/chk-nvidia-clocks/chk-nvidia-clocks.sh
timeout     = 10

[tests.chk-nvidia-smi]
description = check possible stuck nvidia-smi
executable  = /opt/ibm/csm/hcdiag/tests/chk-nvidia-smi/chk-nvidia-smi.sh
timeout     = 10

[tests.chk-nvidia-vbios]
description = check nvidia vbios
executable  = /opt/ibm/csm/hcdiag/tests/chk-nvidia-vbios/chk-nvidia-vbios.sh
timeout     = 10

# we assume by default that nvme device will be mounted /nvme mount point
# if not, pass the mount point as argument
[tests.chk-nvme-mount]
description = check if nvme device is mounted
executable  = /opt/ibm/csm/hcdiag/tests/chk-nvme-mount/chk-nvme-mount.sh
timeout     = 10
#args        = /nvme

[tests.chk-nvme]
description = check nvme device vendor and firmware
executable  = /opt/ibm/csm/hcdiag/tests/chk-nvme/chk-nvme.sh
timeout     = 10

[tests.chk-os]
description = check node operating system and kernel version
executable  = /opt/ibm/csm/hcdiag/tests/chk-os/chk-os.sh

# default list of process : csmd nvidia-persistenced nv-hostengine opal-prd automount
[tests.chk-process]
description = check if a list of process are running on the node
executable  = /opt/ibm/csm/hcdiag/tests/chk-process/chk-process.sh
timeout     = 10
#args = csmd nvidia-persistenced nv-hostengine opal-prd automount

[tests.chk-sw-level]
description = check the node software stack levels
executable  = /opt/ibm/csm/hcdiag/tests/chk-sw-level/chk-sw-level.sh
timeout     = 10

[tests.chk-sys-firmware-local]
description = check firmware levels on the node
executable  = /opt/ibm/csm/hcdiag/tests/chk-sys-firmware-local/chk-sys-firmware-local.sh
timeout     = 10

[tests.chk-sys-firmware]
description = check machine firmware levels
executable  = /opt/ibm/csm/hcdiag/tests/chk-sys-firmware/chk-sys-firmware.sh
timeout     = 10
targetType  = Management

[tests.chk-temp]
description = Check the temperature sensors on a node
executable  = /opt/ibm/csm/hcdiag/tests/chk-temp/chk-temp.sh
timeout     = 10

[tests.chk-zombies]
description = check possible stuck zombie tasks
executable  = /opt/ibm/csm/hcdiag/tests/chk-zombies/chk-zombies.sh
timeout     = 10

[tests.compdiag]
description = comprehensive fabric diagnostic test
executable  = /opt/ibm/csm/hcdiag/tests/compdiag/compdiag.sh
group       = ib
timeout     = 600

## IBM spectrum mpi package daxpy
[tests.daxpy]
description = measure aggregate memory bandwidth in MB/s.
executable  = /opt/ibm/csm/hcdiag/tests/daxpy/daxpy.sh
group       = memory
timeout     = 100
targetType  = Compute    

# NVIDIA datacenter. Argment for dcgmi diag is level, possible values:
# 1 - Quick System Validation (~ seconds)  
# 2 - Extended System Validation (~ 2 minutes)  
# 3 - System HW Diagnostics ( ~ 15 minutes)        
[tests.dcgm-diag]
description = NVIDIA DCGM diagnostic test
group       = gpu
timeout     = 900
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/dcgm-diag/dcgm-diag.sh
args        = 3

# NVIDIA datacenter. Argument for dcgmi health is the list of what to watch.
# Default is all.
# Possible values:
# a - all watches
# p - PCIe watches 
# m - memory watches 
# i - infoROM watches
# t - thermal and power watches 
# n - NVLink watches 
[tests.dcgm-health]
description = NVIDIA DCGM health check
group       = gpu
timeout     = 100
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/dcgm-health/dcgm-health.sh
#args        = a

##  IBM spectrum mpi package dgemm
[tests.dgemm]
description = measure CPU performance. 
executable  = /opt/ibm/csm/hcdiag/tests/dgemm/dgemm.sh
group       = cpu
timeout     = 600
targetType  = Compute

##  IBM spectrum mpi package dgemm-cpu
[tests.dgemm-gpu]
description  = measure GPU performance.
executable  = /opt/ibm/csm/hcdiag/tests/dgemm-gpu/dgemm-gpu.sh
group        = gpu
timeout      = 2000
targetType   = Compute

# strong recomend to run on the node, just invoking this script
# NOTE: NVIDIA distributes the fieldiag binary and the source code for the
#       kernel module. The files should be copied to /opt/ibm/csm/hcdiag/tests/fieldiag 
#       or the path should be updated in the fieldiag.sh script.
[tests.fieldiag]
description = NVIDIA hardware diagnostic
executable  = /opt/ibm/csm/hcdiag/tests/fieldiag/fieldiag.sh
group       = gpu
timeout     = 25000

# NOTE: Distributed by GPFS
[tests.gpfsperf]
description = GPFS benchmark to measure the performance of the filesystem
executable  = /opt/ibm/csm/hcdiag/tests/gpfsperf/gpfsperf.sh
group       = Filesystem
#             <gpfs_fs>   [blocksize]  [directory]
args        = /gpfs/gpfs0   1M  /gpfs/gpfs0/diagadmin 
timeout     = 25000

# checks gpu memory bw, gpu dgemm flops, nvlink transfer speeds 
# NOTE: the source code for the gpu-health binary is distributed as samples
# arguments:
#  nvlink transfer to devices in the same socket speed: default 40 GB/sec  
#  gpu memory bandwidth: defaul 800 GB/sec
#  gpu dgemm: deault is 7 TFlops
[tests.gpu-health]
description = NVIDIA hardware diagnostic
executable  = /opt/ibm/csm/hcdiag/tests/gpu-health/gpu-health.sh
group       = gpu
timeout     = 10000
#args       = 40 800 7

[tests.gpudirect]
description = Exercise GPUDirect RDMA and p2p data movement
executable  = /opt/ibm/csm/hcdiag/tests/gpudirect/gpudirect.sh
targetType  = Management
group       = ib
timeout     = 150

[tests.hcatest]
description = Verify operating hca with verbs latency test
executable  = /opt/ibm/csm/hcdiag/tests/hcatest/hcatest.sh
targetType  = Management
group       = ib
timeout     = 150

# IBM HTX package.  htx tests arguments:
# cycles: duration of the run, in cycles
# pass  : 1|2, indicating pass1 or pass2 (not all the tests have pass2)
# debug : 0|1, print debug info or not. Default is 0


# arguments for hxecache is <cycles> [debug]
[tests.hxecache]
description = HTX L1/L2/L3 caches stress test
group       = memory
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxecache/hxecache.sh 
args        = 10 

# arguments for hxecpu is  <cycles> <level> [debug] 
[tests.hxecpu]
description = HTX processor core cpu test
group       = cpu
timeout     = 800
targetType  = Compute
executable = /opt/ibm/csm/hcdiag/tests/hxecpu/hxecpu.sh
args        = 10 1 

# arguments for hxecpu_pass2 is  <cycles> <level> [debug] 
[tests.hxecpu_pass2]
description = HTX processor core cpu test pass2
group       = cpu
timeout     = 800
targetType  = Compute
executable = /opt/ibm/csm/hcdiag/tests/hxecpu/hxecpu.sh
args        = 10 2 

# arguments for hxediag is  <eth|ib> <duration> [args]
# if you you want to specify exactly which interface, it has to be passed as 
# args, otherwise it will test all interfaces.
# example ib 10 mlx5_0

[tests.hxediag_eth]
description = HTX test for Ethernet adapters
group       = io
timeout     = 800
targetType  = Compute
executable = /opt/ibm/csm/hcdiag/tests/hxediag/hxediag.sh
args        = eth 10 

# arguments for hxediag is  <eth|ib> <duration> [args]
[tests.hxediag_ib]
description = HTX test for IB adapters
group       = io
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxediag/hxediag.sh
args        = ib 10 

# arguments for hxeewm is <cycles> [debug]
[tests.hxeewm_pass2]
description = HTX thermal test
group       = processor
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxeewm/hxeewm.sh
args        = 10 1 


# arguments for hxefabricbus  <cycles> [debug] 
[tests.hxefabricbus_pass2]
description = HTX processor fabric bus test 
group       = processor
timeout     = 800
targetType  = Compute
executable = /opt/ibm/csm/hcdiag/tests/hxefabricbus/hxefabricbus.sh
args        = 10  

# arguments for hxefpu64 is <cycles> <level> [debug]
[tests.hxefpu64]
description = HTX VSU (Vector Scalar Unit) test
group       = processor
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxefpu64/hxefpu64.sh
args        = 10 1 

# arguments for hxefpu64_pass2 is <cycles> <level> [debug]
[tests.hxefpu64_pass2]
description = HTX VSU (Vector Scalar Unit) test pass2
group       = processor
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxefpu64/hxefpu64.sh
args        = 10 2 

# arguments for hxemem64 is  <cycles> <level> [debug]
[tests.hxemem64]
description = HTX memory subsystem test
group       = memory
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxemem64/hxemem64.sh
args        = 2 1 

# arguments for hxemem64_pass2 is <cycles> <level> [debug]
[tests.hxemem64_pass2]
description = HTX memory subsystem test
group       = memory
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxemem64/hxemem64.sh
args        = 2 2

# arguments for hxenvidia is <cycles> [debug]
[tests.hxenvidia]
description = HTX gpu test
group       = gpu
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxenvidia/hxenvidia.sh
args        = 10 1

# arguments for hxenvidia is <cycles> [debug]
[tests.hxenvidia_pass2]
description = HTX gpu test
group       = gpu
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxenvidia/hxenvidia.sh
args        = 10 2

# arguments for hxerng is <cycles> [debug]
[tests.hxerng_pass2]
description = HTX random number generator engine test
group       = core
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxerng/hxerng.sh
args        = 10 

# arguments for hxesctu is <cycles> [debug]
[tests.hxesctu_pass2]
description = HTX processor cache coherency test
group       = memory
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxesctu/hxesctu.sh
args        = 10  

# arguments for hxestorage is <module> <duration> <pass> 
# disk
[tests.hxestorage_sd]
description = HTX storage subsystem test
group       = storage
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxestorage/hxestorage.sh
args        = sd 10 1  

[tests.hxestorage_sd_pass2]
description = HTX storage subsystem test
group       = storage
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxestorage/hxestorage.sh
args        = sd 10 2  

# nvme
[tests.hxestorage_nvme]
description = HTX storage subsystem test
group       = storage
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxestorage/hxestorage.sh
args        = nvme 10 1  

[tests.hxestorage_nvme_pass2]
description = HTX storage subsystem test
group       = storage
timeout     = 800
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/hxestorage/hxestorage.sh
args        = nvme 10 2  

[tests.ibcredit]
description = Checks for credit loops
executable  = /opt/ibm/csm/hcdiag/tests/ibcredit/ibcredit.sh
group       = ib
timeout     = 10

[tests.ibverbs]
description = Verify verbs point to point communication between nodes
executable  = /opt/ibm/csm/hcdiag/tests/ibverbs/ibverbs.sh
targetType  = Management
group       = ib
timeout     = 150

[tests.ipoib]
description = Shows the state, MTU, and mode of IPoIB devices on a node
executable  = /opt/ibm/csm/hcdiag/tests/ipoib/ipoib.sh
timeout     = 10

[tests.lnklwls]
description = Verify link width and speed
executable  = /opt/ibm/csm/hcdiag/tests/lnklwls/lnklwls.sh
group       = ib
timeout     = 10

[tests.lnkqual]
description = Check for marginal link error
executable  = /opt/ibm/csm/hcdiag/tests/lnkqual/lnkqual.sh
group       = ib
timeout     = 10

##  IBM spectrum mpi package jlink
[tests.jlink]
description = JLINK measures the bandwidth between nodes, and to discover bad or low-performing links.
executable  = /opt/ibm/csm/hcdiag/tests/jlink/jlink.sh
group       = ib
timeout     = 50
targetType  = Compute
clustertest = yes
          
[tests.nsdperf]
description = network performance test
group       = performance
timeout     = 300
targetType  = Management
executable  = /opt/ibm/csm/hcdiag/tests/nsdperf/nsdperf.sh

# NVIDIA nvvs. Argument for nvvs is the same as dcgm-diag arguments
[tests.nvvs]
description = NVVS (NVIDIA Validation Suite).
group       = gpu
timeout     = 300
#timeout     = 100
targetType  = Compute
executable  = /opt/ibm/csm/hcdiag/tests/nvvs/nvvs.sh
args        = 3

## Invokes p2pBandwidthLatencyTest, from cuda samples
[tests.p2pBandwidthLatencyTest]
description = parallel ping from nodes to other nodes in the cluster
executable  = /opt/ibm/csm/hcdiag/tests/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.sh
timeout     = 200
group       = gpu

## Invokes /opt/xcat/bin/ppping
[tests.ppping]
description = parallel ping from nodes to other nodes in the cluster
executable  = /opt/ibm/csm/hcdiag/tests/ppping/ppping.sh
targetType  = Management
timeout     = 200
group       = ib
args        = -i ib0,ib1

[tests.rpower]
description = check rpower status
executable  = /opt/ibm/csm/hcdiag/tests/rpower/rpower.sh
timeout     = 100
targetType  = Management
xcat_cmd    = yes
#args        = -v

[tests.rvitals]
description = check if rvitals match the spec 
executable  = /opt/ibm/csm/hcdiag/tests/rvitals/rvitals.sh
timeout     = 300
targetType  = Management
xcat_cmd    = yes
#args        = -v

[tests.swhealth]
description = Run switch health check report
executable  = /opt/ibm/csm/hcdiag/tests/swhealth/swhealth.sh
timeout     = 60

## very simple hello test
[tests.test-simple]
description = simple hello test
executable  = /opt/ibm/csm/hcdiag/tests/test-simple/test-simple.sh
timeout     = 1

[bucket.node_health]
description = A default set of tests designed to provide comprehensive node_health.
tests       = chk-memory

[bucket.performance]
description = A default set of tests designed to provide comprehensive performance test.
tests       = dgemm, daxpy, jlink

[bucket.ib]    
description = A default set of tests to provide comprehensive Infiniband test.
tests       = jlink, ppping

[bucket.gpu]    
description = A default set of tests to provide comprehensive GPU test.
tests       = dcgm-diag


[bucket.htx_pass1]    
description = A bucket including all htx pass1 tests
tests       = hxecache,hxecpu,hxefpu64,hxemem64,hxenvidia,hxediag_ib, hxediag_eth,hxestorage_sd,hxestorage_nvme

[bucket.htx_pass2]    
description = A bucket including all htx pass2 tests
tests       = hxeewm_pass2,hxecpu_pass2,hxefpu64_pass2,hxemem64_pass2,hxenvidia_pass2,hxerng_pass2,hxesctu_pass2,hxefabricbus_pass2,hxestorage_sd_pass2,hxestorage_nvme_pass2
